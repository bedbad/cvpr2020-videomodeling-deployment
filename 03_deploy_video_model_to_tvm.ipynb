{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy video models to TVM runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires gluoncv>=0.8.0 alpha: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import gluoncv as gcv\n",
    "import mxnet as mx\n",
    "import tvm\n",
    "print('Requires gluoncv>=0.8.0 alpha:', gcv.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's grab a pretrained model from GluonCV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet18_v1b_kinetics400'\n",
    "net = gcv.model_zoo.get_model(model_name, nclass=400, pretrained=True, ctx=mx.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case you missed the concept, we support injecting preprocessing pipeline into the network itself\n",
    "With embedded preprocessing block, it's much easier to consume the model without writing preprocessing code in other languages such as C++/JAVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import HybridBlock\n",
    "class TvmPreprocess(HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TvmPreprocess, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            mean = mx.nd.array([123.675, 116.28, 103.53]).reshape((1, 3, 1, 1))\n",
    "            scale = mx.nd.array([58.395, 57.12, 57.375]).reshape((1, 3, 1, 1))\n",
    "            self.init_mean = self.params.get_constant('init_mean', mean)\n",
    "            self.init_scale = self.params.get_constant('init_scale', scale)\n",
    "\n",
    "    # pylint: disable=arguments-differ\n",
    "    def hybrid_forward(self, F, x, init_mean, init_scale):\n",
    "        x = F.broadcast_minus(x, init_mean)\n",
    "        x = F.broadcast_div(x, init_scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to TVM is similar to the process of generating mxnet static network representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find config for target=cuda, workload=('conv2d_nchw.cuda', ('TENSOR', (1, 64, 56, 56), 'float32'), ('TENSOR', (64, 64, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=cuda, workload=('conv2d_nchw.cuda', ('TENSOR', (1, 128, 28, 28), 'float32'), ('TENSOR', (128, 128, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=cuda, workload=('conv2d_nchw.cuda', ('TENSOR', (1, 256, 14, 14), 'float32'), ('TENSOR', (256, 256, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=cuda, workload=('conv2d_nchw.cuda', ('TENSOR', (1, 512, 7, 7), 'float32'), ('TENSOR', (512, 512, 3, 3), 'float32'), (1, 1), (1, 1, 1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.\n",
      "Cannot find config for target=cuda, workload=('dense_small_batch.cuda', ('TENSOR', (1, 512), 'float32'), ('TENSOR', (400, 512), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n"
     ]
    }
   ],
   "source": [
    "gcv.utils.export_tvm(model_name, net, data_shape=(1, 3, 224, 224), preprocess=TvmPreprocess(), \n",
    "                     target='cuda', ctx=mx.gpu(0), opt_level=3, use_autotvm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile we can save the synset to disk so we can map categories to original names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}_synset.txt'.format(model_name), 'wt') as f:\n",
    "    for c in net.classes:\n",
    "        f.write(c + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 xavier xavier  73M Jun 10 20:25 resnet18_v1b_kinetics400_deploy_0000.params\r\n",
      "-rw-rw-r-- 1 xavier xavier  23K Jun 10 20:25 resnet18_v1b_kinetics400_deploy_graph.json\r\n",
      "-rwxrwxr-x 1 xavier xavier 477K Jun 10 20:25 resnet18_v1b_kinetics400_deploy_lib.so\r\n",
      "-rw-rw-r-- 1 xavier xavier 5.9K Jun 10 20:25 resnet18_v1b_kinetics400_synset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $model_name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
